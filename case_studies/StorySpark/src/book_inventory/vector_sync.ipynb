{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f651d1cc-882e-4043-b694-4e18d24aa647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Install SDKs (run once per cluster)\n",
    "%pip install databricks-sdk[openai]\n",
    "%pip install databricks-sdk[openai] databricks-vectorsearch\n",
    "\n",
    "# 2. Imports and client initialization\n",
    "from pyspark.sql.functions import col\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# 3. Clients\n",
    "wclient = WorkspaceClient()\n",
    "openai_client = wclient.serving_endpoints.get_open_ai_client()\n",
    "vs_client = VectorSearchClient()\n",
    "\n",
    "# 4. Parameters\n",
    "DELTA_TABLE       = \"prod.storyspark.book_inventory\"\n",
    "VECTOR_ENDPOINT   = \"storyspark_book_inventory_endpoint\"\n",
    "VECTOR_NAMESPACE  = \"prod.storyspark.book_inventory_index\"\n",
    "MODEL_NAME        = \"databricks-gte-small-en\"\n",
    "WATERMARK_TABLE   = \"prod.storyspark.book_inventory_watermark\"\n",
    "BATCH_SIZE        = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67165888-9ed4-4eeb-bb63-0dc4413af01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Create Watermark Control Table (If Not Exists)\n",
    "--This table records the last updated_at timestamp you processed.\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS prod.storyspark.book_inventory_watermark (\n",
    "  last_timestamp TIMESTAMP\n",
    ")\n",
    "USING DELTA;\n",
    "\n",
    "-- Seed a baseline row if the table is empty\n",
    "INSERT INTO prod.storyspark.book_inventory_watermark\n",
    "SELECT to_timestamp('1970-01-01 00:00:00')\n",
    "WHERE NOT EXISTS (SELECT 1 FROM prod.storyspark.book_inventory_watermark);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee469fb-cb37-4984-8a04-cde0ef7fc91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Last Processed Timestamp\n",
    "\n",
    "watermark_df = spark.table(WATERMARK_TABLE).limit(1)\n",
    "last_timestamp = watermark_df.collect()[0][\"last_timestamp\"]\n",
    "print(f\"Last sync time: {last_timestamp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f07d89-e106-4611-9e6f-8df9478b53de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read New or Updated Rows\n",
    "\n",
    "new_rows_df = (\n",
    "    spark.table(DELTA_TABLE)\n",
    "         .filter(col(\"last_updated_at\") > last_timestamp)\n",
    "         #.select(\"book_id\", \"owner_id\", \"relevant_text\", \"last_read\", \"last_updated_at\")\n",
    ")\n",
    "\n",
    "count = new_rows_df.count()\n",
    "print(f\"Found {count} new/updated rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbcec9e-83e2-43b2-84c5-d0d82cf29ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Embedding Helper\n",
    "\n",
    "def embed_texts(texts: list[str]) -> list[list[float]]:\n",
    "    # Batch call to reduce request overhead\n",
    "    resp = openai_client.embeddings.create(\n",
    "        model=MODEL_NAME,\n",
    "        input=texts\n",
    "    )\n",
    "    return [item.embedding for item in resp.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "886410e2-6488-4850-9760-0208769f7376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build & Upsert Vector Batches\n",
    "rows = new_rows_df.collect()\n",
    "vectors_payload = []\n",
    "\n",
    "for row in rows:\n",
    "    embedding = embed_texts([rows.relevant_text])[0]\n",
    "    vectors_payload.append({\n",
    "        \"id\":       row.book_id,\n",
    "        \"values\":   embedding,\n",
    "        \"metadata\": {\n",
    "            \"owner_id\": row.owner_id,\n",
    "            \"last_read\": str(row.last_read)\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Upsert in batches\n",
    "for start in range(0, len(vectors_payload), BATCH_SIZE):\n",
    "    batch = vectors_payload[start : start + BATCH_SIZE]\n",
    "    vs_client.upsert_vectors(\n",
    "        endpoint_name=VECTOR_ENDPOINT,\n",
    "        namespace=VECTOR_NAMESPACE,\n",
    "        vectors=batch\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50975bf1-9e39-43ad-aa6d-44a987e10d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update Watermark\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType\n",
    "\n",
    "if rows:\n",
    "    # Compute the maximum updated_at we just processed\n",
    "    new_max_timestamp = max(r[\"last_updated_at\"] for r in rows)\n",
    "else:\n",
    "    # No new rows â†’ keep old watermark\n",
    "    new_max_timestamp = last_timestamp\n",
    "\n",
    "last_timestamp_schema = StructType([\n",
    "    StructField(\"last_timestamp\", TimestampType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Overwrite the watermark table\n",
    "spark.createDataFrame(\n",
    "    [(new_max_timestamp,)],\n",
    "    last_timestamp_schema\n",
    ").write.mode(\"overwrite\").saveAsTable(WATERMARK_TABLE)\n",
    "\n",
    "print(f\"Watermark updated to: {new_max_timestamp}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7762384774322383,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "vector_sync",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
